---
title: LLM Benchmark for Arm Server
description: This self-service project sets up a reproducible MLPerf Inference workflow to benchmark large-language-model performance across Arm server configurationsâ€”yielding hard data that guides optimization of Arm hardware and software stacks for AI workloads.
subjects:
- ML
- Performance and Architecture
requires-team:
- No
platform:
- Servers and Cloud Computing
- Laptops and Desktops
- AI
sw-hw:
- Software
support-level:
- Self-Service
- Arm Ambassador Support
publication-date: 2025-05-30
license:
status:
- Hidden
donation:
layout: article
sidebar:
  nav: projects
full_description: |-
  ## Description
  This project aims to benchmark inference on Arm-based servers using the MLPerf Inference benchmark suite. The project spans performance analysis across different configurations of Arm-based servers. The main deliverable is a comprehensive benchmarking setup that can evaluate the performance of large language models (LLMs) on various Arm server configurations in addition to a report highlighting the performance difference and how to recreate the results. This project will provide practical experience in benchmarking, performance analysis, and working with Arm-based server architectures. The final output will be a detailed report and a functional benchmarking infrastructure that can be used for further research and development.


  ## Prequisites

  - Intermediate understanding of Python and C++
  - Intemediate understanding of ML frameworks such as MLPerf, TensorFlow and PyTorch
  - Access to physcial Arm-based server or access to cloud service providers

  ## Resources from Arm and our partners

  - Repository: [MLPerf Inference ](https://github.com/mlcommons/inference)
  - External Documentation: [MLPerf Inference Benchmark Suite](https://mlcommons.org/en/inference-datacenter-20/)
  - Blog: [Arm Server inference performance](https://community.arm.com/arm-community-blogs/b/servers-and-cloud-computing-blog/posts/machine-learning-inference-on-aws-graviton3)

  ## Support Level

  This project is designed to be self-serve but comes with opportunity of some community support from Arm Ambassadors, who are part of the Arm Developer program. If you are not already part of our program, [click here to join](https://www.arm.com/resources/developer-program?#register).

  ## Benefits 

  Standout project contributions will result in preferential internal referrals to Arm Talent Acquisition (with digital badges for CV building).  And we are currently discussing with national agencies the potential for funding streams for Arm Developer Labs projects, which would flow to you, not us.

  To receive the benefits, you must show us your project through our [online form](https://forms.office.com/e/VZnJQLeRhD). Please do not include any confidential information in your contribution. Additionally if you are affiliated with an academic institution, please ensure you have the right to share your material.
---
## Description
This project aims to benchmark inference on Arm-based servers using the MLPerf Inference benchmark suite. The project spans performance analysis across different configurations of Arm-based servers. The main deliverable is a comprehensive benchmarking setup that can evaluate the performance of large language models (LLMs) on various Arm server configurations in addition to a report highlighting the performance difference and how to recreate the results. This project will provide practical experience in benchmarking, performance analysis, and working with Arm-based server architectures. The final output will be a detailed report and a functional benchmarking infrastructure that can be used for further research and development.


## Prequisites

- Intermediate understanding of Python and C++
- Intemediate understanding of ML frameworks such as MLPerf, TensorFlow and PyTorch
- Access to physcial Arm-based server or access to cloud service providers

## Resources from Arm and our partners

- Repository: [MLPerf Inference ](https://github.com/mlcommons/inference)
- External Documentation: [MLPerf Inference Benchmark Suite](https://mlcommons.org/en/inference-datacenter-20/)
- Blog: [Arm Server inference performance](https://community.arm.com/arm-community-blogs/b/servers-and-cloud-computing-blog/posts/machine-learning-inference-on-aws-graviton3)

## Support Level

This project is designed to be self-serve but comes with opportunity of some community support from Arm Ambassadors, who are part of the Arm Developer program. If you are not already part of our program, [click here to join](https://www.arm.com/resources/developer-program?#register).

## Benefits 

Standout project contributions will result in preferential internal referrals to Arm Talent Acquisition (with digital badges for CV building).  And we are currently discussing with national agencies the potential for funding streams for Arm Developer Labs projects, which would flow to you, not us.

To receive the benefits, you must show us your project through our [online form](https://forms.office.com/e/VZnJQLeRhD). Please do not include any confidential information in your contribution. Additionally if you are affiliated with an academic institution, please ensure you have the right to share your material.